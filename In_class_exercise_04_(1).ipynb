{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_04 (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Explorer9/prabhu_INFO5731/blob/master/In_class_exercise_04_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/INFO5731_FALL2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vR0L3_CreM_A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "3dd10025-c3cf-4a8b-97b7-1dfa92ba6e08"
      },
      "source": [
        "# Write your code here\n",
        "import requests\n",
        "opener=open('/content/01-05-1 Adams v Tanner.txt','r')\n",
        "text=opener.read()\n",
        "#print(text)\n",
        "\n",
        "1.1\n",
        "#1\n",
        "print(\"Number of sentences\",text.count('.'))\n",
        "\n",
        "#2\n",
        "words=text.split(\" \")\n",
        "print(\"Number of words\",len(words))\n",
        "\n",
        "#3\n",
        "\n",
        "print(\"Number of characters:\",len(text))\n",
        "\n",
        "#4\n",
        "word_length=0\n",
        "for i in words:\n",
        "  word_length=word_length+len(i)\n",
        "print(\"Average word length\",word_length/len(words))\n",
        "\n",
        "#5\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_count=0\n",
        "stop = stopwords.words('english')\n",
        "for i in text.split(\" \"):\n",
        "  if i in stop:\n",
        "    stop_count=stop_count+1\n",
        "print(\" Number of stop words:\",stop_count)\n",
        "\n",
        "#6 \n",
        "special_count=0\n",
        "special_characters=['#','@','?','*','!','$','%','^','&','(',')']\n",
        "for i in text:\n",
        "  if i in special_characters:\n",
        "    special_count=special_count+1\n",
        "print(\"Numbe of special characters:\",special_count)\n",
        "\n",
        "#7\n",
        "numeric_count=0\n",
        "for i in text:\n",
        "  if i.isdigit():\n",
        "    numeric_count+=1\n",
        "print(\"number of numeric:\",numeric_count)\n",
        "\n",
        "#8\n",
        "upper_count=0\n",
        "for i in text:\n",
        "  if i.isupper():\n",
        "    upper_count+=1\n",
        "print(\"number of upper:\",upper_count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences 291\n",
            "Number of words 3576\n",
            "Number of characters: 20453\n",
            "Average word length 4.719798657718121\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            " Number of stop words: 1671\n",
            "Numbe of special characters: 43\n",
            "number of numeric: 356\n",
            "number of upper: 695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc_xsF1rXW45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "outputId": "2b2cef33-0068-486d-cf05-8c273c767fb4"
      },
      "source": [
        "1.2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "sentence_list=nltk.tokenize.sent_tokenize(text)\n",
        "d={'sentences':sentence_list}\n",
        "df=pd.DataFrame(data=d)\n",
        "#1\n",
        "df['sentences'] = df['sentences'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "print(\"lower casing: \",df['sentences'].head())\n",
        "\n",
        "#2\n",
        "df['sentences'] = df['sentences'].str.replace('[^\\w\\s]','')\n",
        "print(\"punctuaion removal\",df['sentences'].head())\n",
        "\n",
        "#3\n",
        "df['sentences'] = df['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "print(\"stopwords:\",df['sentences'].head())\n",
        "\n",
        "#4\n",
        "freq = pd.Series(' '.join(df['sentences']).split()).value_counts()[:10]\n",
        "freq=list(freq.index)\n",
        "df['sentences'] = df['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "print(\"frequent words\",df['sentences'].head())\n",
        "\n",
        "#5\n",
        "freq_lower = pd.Series(' '.join(df['sentences']).split()).value_counts()[-10:]\n",
        "freq_lower=list(freq_lower.index)\n",
        "df['sentences'] = df['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_lower))\n",
        "print(\"rare words removal\",df['sentences'].head())\n",
        "\n",
        "#6\n",
        "\n",
        "from textblob import TextBlob\n",
        "df['sentences'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
        "print(\"spelling correction\",df['sentences'].head())\n",
        "#7\n",
        "print(TextBlob(df['sentences'][1]).words)\n",
        "\n",
        "#8\n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "print(\"stemming\",df['sentences'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()])))\n",
        "\n",
        "#9\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df['sentences'] = df['sentences'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "print(\"lemmeatiaztion\",df['sentences'].head())\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "lower casing:  0                 5 ala. 740 supreme court of alabama.\n",
            "1                          adams v. tanner and horton.\n",
            "2                                     june term, 1843.\n",
            "3    synopsis writ of error to the circuit court of...\n",
            "4    west headnotes (2) [1] chattel mortgages crops...\n",
            "Name: sentences, dtype: object\n",
            "punctuaion removal 0                   5 ala 740 supreme court of alabama\n",
            "1                            adams v tanner and horton\n",
            "2                                       june term 1843\n",
            "3    synopsis writ of error to the circuit court of...\n",
            "4    west headnotes 2 1 chattel mortgages crops a g...\n",
            "Name: sentences, dtype: object\n",
            "stopwords: 0                      5 ala 740 supreme court alabama\n",
            "1                                adams v tanner horton\n",
            "2                                       june term 1843\n",
            "3             synopsis writ error circuit court sumter\n",
            "4    west headnotes 2 1 chattel mortgages crops gro...\n",
            "Name: sentences, dtype: object\n",
            "frequent words 0                            5 ala 740 supreme alabama\n",
            "1                                  adams tanner horton\n",
            "2                                       june term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgages crops gro...\n",
            "Name: sentences, dtype: object\n",
            "rare words removal 0                            5 ala 740 supreme alabama\n",
            "1                                  adams tanner horton\n",
            "2                                            term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgages crops gro...\n",
            "Name: sentences, dtype: object\n",
            "spelling correction 0                            5 ala 740 supreme alabama\n",
            "1                                  adams tanner horton\n",
            "2                                            term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgages crops gro...\n",
            "Name: sentences, dtype: object\n",
            "['adams', 'tanner', 'horton']\n",
            "stemming 0                             5 ala 740 suprem alabama\n",
            "1                                   adam tanner horton\n",
            "2                                            term 1843\n",
            "3                    synopsi writ error circuit sumter\n",
            "4    west headnot 2 1 chattel mortgag crop grow exi...\n",
            "Name: sentences, dtype: object\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "lemmeatiaztion 0                            5 ala 740 supreme alabama\n",
            "1                                   adam tanner horton\n",
            "2                                            term 1843\n",
            "3                   synopsis writ error circuit sumter\n",
            "4    west headnotes 2 1 chattel mortgage crop growi...\n",
            "Name: sentences, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPcFy9PNO5IC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1.3\n",
        "\n",
        "df.to_csv('out_csv.csv',index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zy6sSZc34Ez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "d005d0ce-d0e4-4c74-943a-08f90860f8fb"
      },
      "source": [
        "#1.4\n",
        "freq = pd.Series(' '.join(df['sentences']).split()).value_counts()\n",
        "print(freq)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rep          16\n",
            "contract     16\n",
            "growing      16\n",
            "ala          15\n",
            "plaintiff    15\n",
            "             ..\n",
            "claimed       1\n",
            "meant         1\n",
            "value         1\n",
            "purchaser     1\n",
            "stated        1\n",
            "Length: 782, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_tYL_O04rHm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "c3e97036-889c-4d88-db83-4fea9e009aa9"
      },
      "source": [
        "1.4 \n",
        "#N-grams\n",
        "TextBlob(df['sentences'][0]).ngrams(1)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['5']),\n",
              " WordList(['ala']),\n",
              " WordList(['740']),\n",
              " WordList(['supreme']),\n",
              " WordList(['alabama'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5fJIvbU5Wxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e28a4e77-4739-418e-9717-d52896943074"
      },
      "source": [
        "TextBlob(df['sentences'][0]).ngrams(2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['5', 'ala']),\n",
              " WordList(['ala', '740']),\n",
              " WordList(['740', 'supreme']),\n",
              " WordList(['supreme', 'alabama'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgh7fRom5Y6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "be7f0563-2538-478b-8aa8-6c311e46573e"
      },
      "source": [
        "TextBlob(df['sentences'][0]).ngrams(3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['5', 'ala', '740']),\n",
              " WordList(['ala', '740', 'supreme']),\n",
              " WordList(['740', 'supreme', 'alabama'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wSv6fVhOfFmv",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "import re\n",
        "ip = \"260.08.094.109\"\n",
        "new_ip=\"\"\n",
        "for i in ip.split(\".\"):\n",
        "  if re.findall('^0',i):\n",
        "    while re.findall('^0',i):\n",
        "      i=i[1:]\n",
        "    new_ip=new_ip+\".\"+i\n",
        "  else:\n",
        "    new_ip=new_ip+\".\"+i\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWv2cwST8pkz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "765f33e7-a43c-405e-8ed7-db8808a7ed36"
      },
      "source": [
        "new_ip"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'.260.8.94.109'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7xdJpDx9gjbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3106716-02b1-45d3-c93d-1099564ed8c7"
      },
      "source": [
        "# Write your code here\n",
        "sentence=\"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
        "re.findall(\"[1-9][0-9][0-9][0-9]\",sentence)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2010', '1000', '2010', '2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}